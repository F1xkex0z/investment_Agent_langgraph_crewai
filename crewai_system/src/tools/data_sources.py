"""
æ•°æ®æºæ¥å£é€‚é…å™¨
é›†æˆåŸç³»ç»Ÿçš„çœŸå®æ•°æ®è·å–åŠŸèƒ½
"""

import sys
import os
import time
import importlib
import akshare as ak
import pandas as pd
import requests
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union
import numpy as np
import json

# æ·»åŠ åŸç³»ç»Ÿè·¯å¾„ä»¥ä¾¿å¯¼å…¥ç°æœ‰å·¥å…·
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from crewai_system.src.utils.logging_config import get_logger, log_info, log_error, log_success, log_failure, log_performance, log_data_collection, log_api_call, log_market_data
from crewai.tools import BaseTool
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from crewai_system.src.config import config


class DataSourceAdapter:
    """æ•°æ®æºé€‚é…å™¨ï¼Œå°è£…åŸç³»ç»Ÿçš„æ•°æ®è·å–åŠŸèƒ½"""

    def __init__(self):
        # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
        self.logger = get_logger("data")
        self.debug_logger = get_logger("debug")
        self._cache = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        # å°è¯•å¯¼å…¥çœŸå®æ•°æ®æº
        self._import_real_data_sources()

    def _log_data_collection(self, data_type: str, identifier: str, data_size: int,
                           execution_time: float, success: bool, error: str = None):
        """è®°å½•æ•°æ®æ”¶é›†æ€§èƒ½ä¿¡æ¯"""
        status = "æˆåŠŸ" if success else "å¤±è´¥"
        error_info = f", error={error}" if error else ""
        self.logger.info(f"{data_type}æ”¶é›†{status}: {identifier}, data_size={data_size}, execution_time={execution_time:.2f}s{error_info}")

    def _import_real_data_sources(self):
        """å¯¼å…¥çœŸå®æ•°æ®æº"""
        start_time = datetime.now()
        self.logger.debug(f"å¼€å§‹å¯¼å…¥æ•°æ®æºï¼Œæ—¶é—´: {start_time.isoformat()}")

        try:
            # å¯¼å…¥akshare
            import akshare as ak
            self.ak = ak
            self.logger.info("æˆåŠŸå¯¼å…¥akshare")
        except ImportError:
            self.logger.error("akshareä¸å¯ç”¨ï¼Œç³»ç»Ÿæ— æ³•æ­£å¸¸è¿è¡Œ")
            raise ImportError("akshareä¸å¯ç”¨ï¼Œè¯·å®‰è£…akshareåŒ…")

        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # å°è¯•å¯¼å…¥åŸç³»ç»Ÿçš„APIæ¨¡å—
        try:
            # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
            project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
            if project_root not in sys.path:
                sys.path.insert(0, project_root)

            # å°è¯•ç›´æ¥å¯¼å…¥ï¼Œç»•è¿‡Pythonæ¨¡å—ç³»ç»Ÿ
            import importlib.util

            # æ„å»ºAPIæ¨¡å—çš„å®Œæ•´è·¯å¾„
            api_path = os.path.join(project_root, 'src', 'tools', 'api.py')
            if os.path.exists(api_path):
                self.logger.info(f"æ‰¾åˆ°APIæ–‡ä»¶: {api_path}")
                # åŠ¨æ€å¯¼å…¥APIæ¨¡å—
                spec = importlib.util.spec_from_file_location("api_module", api_path)
                api_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(api_module)

                get_financial_metrics = api_module.get_financial_metrics
                get_financial_statements = api_module.get_financial_statements
                get_market_data = api_module.get_market_data
                get_price_history = api_module.get_price_history

                self.logger.info("æˆåŠŸå¯¼å…¥åŸç³»ç»ŸAPIæ¨¡å—")
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°APIæ–‡ä»¶: {api_path}")
                raise ImportError("æ— æ³•æ‰¾åˆ°APIæ–‡ä»¶")
            self.original_api = {
                'get_financial_metrics': get_financial_metrics,
                'get_financial_statements': get_financial_statements,
                'get_market_data': get_market_data,
                'get_price_history': get_price_history
            }
            self.logger.info("æˆåŠŸå¯¼å…¥åŸç³»ç»ŸAPIæ¨¡å—")
        except ImportError as e:
            self.logger.warning(f"æ— æ³•å¯¼å…¥åŸç³»ç»ŸAPIæ¨¡å—: {e}")
            self.original_api = None

        # ç›´æ¥ä½¿ç”¨akshareè·å–æ–°é—»æ•°æ®ï¼Œä¸å†å°è¯•å¯¼å…¥æ–°é—»çˆ¬è™«æ¨¡å—
        self.news_crawler = None

        # è®¾ç½®çœŸå®æ•°æ®è·å–æ–¹æ³•
        self._setup_real_data_methods()

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        self.logger.info("æ•°æ®æºå¯¼å…¥å®Œæˆ")

    def _setup_real_data_methods(self):
        """è®¾ç½®çœŸå®æ•°æ®è·å–æ–¹æ³•"""

        # å¦‚æœæœ‰çœŸå®çš„APIå‡½æ•°ï¼Œä¼˜å…ˆä½¿ç”¨
        if hasattr(self, 'original_api') and self.original_api is not None:
            self.logger.info("ä½¿ç”¨çœŸå®çš„APIå‡½æ•°è·å–æ•°æ®")
            self.get_price_history = self.original_api['get_price_history']
            self.get_financial_metrics = self.original_api['get_financial_metrics']
            self.get_market_data = self.original_api['get_market_data']
            self.get_financial_statements = self.original_api['get_financial_statements']

            # ç›´æ¥ä½¿ç”¨akshareçš„æ–°é—»æœç´¢å‡½æ•°
            def _direct_search_financial_news(keywords: str, num_articles: int = 10) -> List[Dict[str, Any]]:
                """ç›´æ¥ä½¿ç”¨akshareè·å–æ–°é—»æ•°æ®"""
                self.logger.info(f"ğŸ” [DEBUG] å¼€å§‹è·å–æ–°é—»æ•°æ®: keywords={keywords}, num_articles={num_articles}")
                try:
                    # é¦–å…ˆå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æ¥å£
                    if keywords.isdigit() and len(keywords) in [6]:
                        # ç›´æ¥ä½¿ç”¨è‚¡ç¥¨ä»£ç è·å–æ–°é—»
                        self.logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æ¥å£è·å–è‚¡ç¥¨ {keywords} çš„æ–°é—»")
                        company_news_df = ak.stock_news_em(symbol=keywords)
                        source = "ä¸œæ–¹è´¢å¯Œ"
                        self.logger.info(f"ğŸ” [DEBUG] ä¸œæ–¹è´¢å¯Œæ¥å£è¿”å›æ•°æ®å½¢çŠ¶: {company_news_df.shape if not company_news_df.empty else 'ç©ºæ•°æ®'}")
                        if not company_news_df.empty:
                            self.logger.info(f"ğŸ” [DEBUG] ä¸œæ–¹è´¢å¯Œæ•°æ®åˆ—å: {list(company_news_df.columns)}")
                    else:
                        # å¯¹äºéè‚¡ç¥¨ä»£ç çš„å…³é”®è¯ï¼Œä½¿ç”¨è´¢è”ç¤¾å…¨çƒè´¢ç»å¿«è®¯
                        self.logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨è´¢è”ç¤¾å…¨çƒè´¢ç»å¿«è®¯æ¥å£ï¼Œå…³é”®è¯: {keywords}")
                        company_news_df = ak.stock_info_global_cls()
                        source = "è´¢è”ç¤¾"
                        self.logger.info(f"ğŸ” [DEBUG] è´¢è”ç¤¾æ¥å£è¿”å›æ•°æ®å½¢çŠ¶: {company_news_df.shape if not company_news_df.empty else 'ç©ºæ•°æ®'}")
                        if not company_news_df.empty:
                            self.logger.info(f"ğŸ” [DEBUG] è´¢è”ç¤¾æ•°æ®åˆ—å: {list(company_news_df.columns)}")

                        # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–°é—»
                        if not company_news_df.empty:
                            text_columns = ['æ ‡é¢˜', 'å†…å®¹'] if 'å†…å®¹' in company_news_df.columns else ['æ ‡é¢˜']
                            self.logger.info(f"ğŸ” [DEBUG] å¯ç”¨æ–‡æœ¬åˆ—: {text_columns}")
                            mask = False
                            for col in text_columns:
                                if col in company_news_df.columns:
                                    mask |= company_news_df[col].astype(str).str.contains(keywords, case=False, na=False)
                            company_news_df = company_news_df[mask]
                            self.logger.info(f"ğŸ” [DEBUG] å…³é”®è¯è¿‡æ»¤åæ•°æ®å½¢çŠ¶: {company_news_df.shape if not company_news_df.empty else 'ç©ºæ•°æ®'}")

                    # å¤„ç†ç»“æœ
                    news_items = []
                    if not company_news_df.empty:
                        company_news_df = company_news_df.head(num_articles)
                        self.logger.info(f"ğŸ” [DEBUG] é™åˆ¶æ–°é—»æ•°é‡ä¸º {num_articles} æ¡åæ•°æ®å½¢çŠ¶: {company_news_df.shape}")

                        for index, row in company_news_df.iterrows():
                            # æ ¹æ®æ•°æ®æºç¡®å®šå­—æ®µå
                            if source == "ä¸œæ–¹è´¢å¯Œ":
                                title = row.get('æ–°é—»æ ‡é¢˜', f"å…³äº{keywords}çš„è´¢ç»æ–°é—»")
                                content = row.get('æ–°é—»å†…å®¹', title)
                                publish_time = row.get('å‘å¸ƒæ—¶é—´', datetime.now().isoformat())
                                article_source = row.get('æ–‡ç« æ¥æº', source)
                            else:
                                title = row.get('æ ‡é¢˜', f"å…³äº{keywords}çš„è´¢ç»æ–°é—»")
                                content = row.get('å†…å®¹', title)
                                publish_time = row.get('å‘å¸ƒæ—¶é—´', datetime.now().isoformat())
                                article_source = source

                            news_item = {
                                "title": title,
                                "content": content,
                                "publish_time": publish_time,
                                "source": article_source,
                                "importance": "high" if index < 3 else "medium"
                            }
                            news_items.append(news_item)

                        self.logger.info(f"ğŸ” [DEBUG] æˆåŠŸå¤„ç† {len(news_items)} æ¡æ–°é—»")
                        # è®°å½•å‰å‡ æ¡æ–°é—»æ ‡é¢˜ç”¨äºè°ƒè¯•
                        for i, item in enumerate(news_items[:3]):
                            self.logger.info(f"ğŸ” [DEBUG] æ–°é—»{i+1}: {item['title'][:50]}... (æ¥æº: {item['source']})")
                    else:
                        self.logger.warning(f"ğŸ” [DEBUG] æœªæ‰¾åˆ°ä»»ä½•æ–°é—»æ•°æ®")

                    return news_items
                except Exception as e:
                    self.logger.error(f"ğŸ” [DEBUG] è·å–æ–°é—»æ•°æ®å¤±è´¥: {e}")
                    import traceback
                    self.logger.error(f"ğŸ” [DEBUG] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    return []
            self.search_financial_news = _direct_search_financial_news
        else:
            self.logger.error("æ— æ³•å¯¼å…¥çœŸå®çš„APIæ¨¡å—ï¼Œç³»ç»Ÿæ— æ³•æ­£å¸¸è¿è¡Œ")
            raise ImportError("æ— æ³•å¯¼å…¥çœŸå®çš„APIæ¨¡å—ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")


    def get_cache_key(self, func_name: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_data = {
            "func": func_name,
            "kwargs": {k: v for k, v in kwargs.items() if k not in ['password', 'token']}
        }
        return hash(str(sorted(cache_data.items())))

    def get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self._cache:
            cached_time, result = self._cache[cache_key]
            if time.time() - cached_time < self._cache_ttl:
                self.logger.debug(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key}")
                return result
            else:
                del self._cache[cache_key]
        return None

    def cache_result(self, cache_key: str, result: Dict[str, Any]):
        """ç¼“å­˜ç»“æœ"""
        self._cache[cache_key] = (time.time(), result)

    def get_market_data_safe(self, ticker: str, **kwargs) -> Dict[str, Any]:
        """å®‰å…¨è·å–å¸‚åœºæ•°æ®"""
        start_time = datetime.now()
        self.logger.debug(f"å¼€å§‹è·å–å¸‚åœºæ•°æ®ï¼Œè‚¡ç¥¨ä»£ç : {ticker}")
        log_api_call("market_data", "GET", {"ticker": ticker, **kwargs})

        cache_key = self.get_cache_key("get_market_data", ticker=ticker, **kwargs)
        cached_result = self.get_cached_result(cache_key)
        if cached_result:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            log_data_collection("market_data_cache", ticker, len(str(cached_result)), execution_time, "ä»ç¼“å­˜è·å–")
            self.logger.info(f"ä»ç¼“å­˜è·å–å¸‚åœºæ•°æ®: {ticker}, è€—æ—¶: {execution_time:.2f}ç§’")
            return cached_result

        try:
            result = self.get_market_data(ticker, **kwargs)
            self.cache_result(cache_key, result)
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            log_data_collection("market_data_api", ticker, len(str(result)), execution_time, "APIè°ƒç”¨æˆåŠŸ")
            log_market_data(ticker, "akshare_api", len(str(result)))
            self._log_data_collection("å¸‚åœºæ•°æ®", ticker, len(str(result)), execution_time, True)
            return result
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            log_data_collection("market_data_api", ticker, 0, execution_time, f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
            log_api_call("market_data", "GET", {"ticker": ticker, **kwargs}, execution_time, "FAILED")
            self._log_data_collection("å¸‚åœºæ•°æ®", ticker, 0, execution_time, False, str(e))
            self.logger.error(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥ {ticker}: {e}")
            return {"market_cap": 0, "error": str(e)}

    def get_price_history_safe(self, ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
        """å®‰å…¨è·å–ä»·æ ¼å†å²"""
        start_time = datetime.now()
        self.debug_logger.debug(f"è·å–ä»·æ ¼å†å²å¼€å§‹: ticker={ticker}, start_date={start_date}, end_date={end_date}")
        log_api_call("price_history", "GET", {"ticker": ticker, "start_date": start_date, "end_date": end_date})

        cache_key = self.get_cache_key("get_price_history", ticker=ticker, start_date=start_date, end_date=end_date)
        cached_result = self.get_cached_result(cache_key)
        if cached_result:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            result_df = pd.DataFrame(cached_result)
            log_data_collection("price_history_cache", ticker, len(result_df), execution_time, "ä»ç¼“å­˜è·å–")
            self.logger.info(f"ä»·æ ¼å†å²è·å–æˆåŠŸ(ç¼“å­˜): ticker={ticker}, data_size={len(result_df)}, execution_time={execution_time:.2f}s, ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return result_df

        try:
            result = self.get_price_history(ticker, start_date, end_date)
            if result is not None and not result.empty:
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç¼“å­˜
                cache_data = result.to_dict('records')
                self.cache_result(cache_key, cache_data)
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                # è®¡ç®—ä»·æ ¼èŒƒå›´ç”¨äºæ—¥å¿—
                if 'close' in result.columns:
                    price_range = (result['close'].min(), result['close'].max())
                else:
                    price_range = None
                log_data_collection("price_history_api", ticker, len(result), execution_time, "APIè°ƒç”¨æˆåŠŸ")
                log_market_data(ticker, "akshare_api", len(result), price_range)
                self.logger.info(f"ä»·æ ¼å†å²è·å–æˆåŠŸ: ticker={ticker}, data_size={len(result)}, execution_time={execution_time:.2f}s")
            else:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                log_data_collection("price_history_api", ticker, 0, execution_time, "APIè¿”å›ç©ºæ•°æ®")
                self.logger.info(f"ä»·æ ¼å†å²è·å–æˆåŠŸ(ç©ºæ•°æ®): ticker={ticker}, data_size=0, execution_time={execution_time:.2f}s, è¿”å›ç©ºæ•°æ®")
            return result if result is not None else pd.DataFrame()
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            log_data_collection("price_history_api", ticker, 0, execution_time, f"APIè°ƒç”¨å¤±è´¥: {str(e)}")
            log_api_call("price_history", "GET", {"ticker": ticker, "start_date": start_date, "end_date": end_date}, execution_time, "FAILED")
            self.logger.info(f"ä»·æ ¼å†å²è·å–å¤±è´¥: ticker={ticker}, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"è·å–ä»·æ ¼å†å²å¤±è´¥ {ticker}: {e}")
            return pd.DataFrame()

    def get_financial_metrics_safe(self, ticker: str) -> Dict[str, Any]:
        """å®‰å…¨è·å–è´¢åŠ¡æŒ‡æ ‡"""
        start_time = datetime.now()
        self.debug_logger.debug(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¼€å§‹: ticker={ticker}")

        cache_key = self.get_cache_key("get_financial_metrics", ticker=ticker)
        cached_result = self.get_cached_result(cache_key)
        if cached_result:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŒ‡æ ‡è·å–æˆåŠŸ(ç¼“å­˜): ticker={ticker}, data_size={len(str(cached_result))}, execution_time={execution_time:.2f}s, ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return cached_result

        try:
            result = self.get_financial_metrics(ticker)
            self.cache_result(cache_key, result)
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŒ‡æ ‡è·å–æˆåŠŸ: ticker={ticker}, data_size={len(str(result))}, execution_time={execution_time:.2f}s")
            return result
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŒ‡æ ‡è·å–å¤±è´¥: ticker={ticker}, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¤±è´¥ {ticker}: {e}")
            return {}

    def get_financial_statements_safe(self, ticker: str) -> Dict[str, Any]:
        """å®‰å…¨è·å–è´¢åŠ¡æŠ¥è¡¨"""
        start_time = datetime.now()
        self.debug_logger.debug(f"è·å–è´¢åŠ¡æŠ¥è¡¨å¼€å§‹: ticker={ticker}")

        cache_key = self.get_cache_key("get_financial_statements", ticker=ticker)
        cached_result = self.get_cached_result(cache_key)
        if cached_result:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŠ¥è¡¨è·å–æˆåŠŸ(ç¼“å­˜): ticker={ticker}, data_size={len(str(cached_result))}, execution_time={execution_time:.2f}s, ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return cached_result

        try:
            result = self.get_financial_statements(ticker)
            self.cache_result(cache_key, result)
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŠ¥è¡¨è·å–æˆåŠŸ: ticker={ticker}, data_size={len(str(result))}, execution_time={execution_time:.2f}s")
            return result
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è´¢åŠ¡æŠ¥è¡¨è·å–å¤±è´¥: ticker={ticker}, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"è·å–è´¢åŠ¡æŠ¥è¡¨å¤±è´¥ {ticker}: {e}")
            return {}

    def search_financial_news_safe(self, keywords: str, num_articles: int = 10) -> List[Dict[str, Any]]:
        """å®‰å…¨æœç´¢è´¢ç»æ–°é—»"""
        start_time = datetime.now()
        self.logger.info(f"ğŸ” [DEBUG] ===== å¼€å§‹æœç´¢è´¢ç»æ–°é—» =====")
        self.logger.info(f"ğŸ” [DEBUG] keywords={keywords}, num_articles={num_articles}")

        cache_key = self.get_cache_key("search_financial_news", keywords=keywords, num_articles=num_articles)
        cached_result = self.get_cached_result(cache_key)
        if cached_result:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"ğŸ” [DEBUG] ===== ä½¿ç”¨ç¼“å­˜æ•°æ® =====")
            self.logger.info(f"ğŸ” [DEBUG] ç¼“å­˜æ•°æ®å¤§å°: {len(cached_result)}")
            # è®°å½•ç¼“å­˜ä¸­çš„æ–°é—»æ ‡é¢˜åˆ°æ—¥å¿—
            if cached_result:
                for i, news_item in enumerate(cached_result[:3]):  # åªè®°å½•å‰3æ¡ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    title = news_item.get('title', 'æ— æ ‡é¢˜')
                    source = news_item.get('source', 'æœªçŸ¥æ¥æº')
                    self.logger.info(f"ğŸ” [DEBUG] ç¼“å­˜æ–°é—»{i+1}: [{source}] {title}")
            self.logger.info(f"ğŸ” [DEBUG] è´¢ç»æ–°é—»è·å–æˆåŠŸ(ç¼“å­˜): keywords={keywords}, data_size={len(cached_result)}, execution_time={execution_time:.2f}s")
            return cached_result

        try:
            self.logger.info(f"ğŸ” [DEBUG] ===== è°ƒç”¨å®é™…æ–°é—»æœç´¢æ–¹æ³• ======")
            result = self.search_financial_news(keywords, num_articles)
            self.logger.info(f"ğŸ” [DEBUG] æ–°é—»æœç´¢æ–¹æ³•è¿”å›ç»“æœå¤§å°: {len(result) if result else 0}")

            self.cache_result(cache_key, result)
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()

            self.logger.info(f"ğŸ” [DEBUG] ===== æ–°é—»æœç´¢å®Œæˆ =====")
            # è®°å½•è·å–çš„æ–°é—»æ ‡é¢˜åˆ°æ—¥å¿—
            if result:
                self.logger.info(f"ğŸ” [DEBUG] å¼€å§‹è®°å½•æ–°é—»è¯¦æƒ…:")
                for i, news_item in enumerate(result[:5]):  # è®°å½•å‰5æ¡
                    title = news_item.get('title', 'æ— æ ‡é¢˜')
                    source = news_item.get('source', 'æœªçŸ¥æ¥æº')
                    publish_time = news_item.get('publish_time', 'æœªçŸ¥æ—¶é—´')
                    content_length = len(news_item.get('content', ''))
                    self.logger.info(f"ğŸ” [DEBUG] æ–°é—»{i+1}: [{source}] {title[:80]}... (å‘å¸ƒæ—¶é—´: {publish_time}, å†…å®¹é•¿åº¦: {content_length})")
            else:
                self.logger.warning(f"ğŸ” [DEBUG] æ–°é—»æœç´¢ç»“æœä¸ºç©º!")

            self.logger.info(f"ğŸ” [DEBUG] è´¢ç»æ–°é—»è·å–æˆåŠŸ: keywords={keywords}, data_size={len(result)}, execution_time={execution_time:.2f}s")
            return result
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.error(f"ğŸ” [DEBUG] ===== æ–°é—»æœç´¢å¼‚å¸¸ =====")
            self.logger.error(f"ğŸ” [DEBUG] è´¢ç»æ–°é—»è·å–å¤±è´¥: keywords={keywords}, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"ğŸ” [DEBUG] æœç´¢è´¢ç»æ–°é—»å¤±è´¥ {keywords}: {e}")
            import traceback
            self.logger.error(f"ğŸ” [DEBUG] è¯¦ç»†é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []

    def get_macro_data(self) -> Dict[str, Any]:
        """è·å–å®è§‚ç»æµæ•°æ®"""
        start_time = datetime.now()
        self.debug_logger.debug(f"è·å–å®è§‚æ•°æ®å¼€å§‹: timestamp={start_time.isoformat()}")

        try:
            # å¦‚æœakå¯ç”¨ï¼Œå°è¯•è·å–çœŸå®æ•°æ®
            if hasattr(self, 'ak') and self.ak is not None:
                try:
                    self.debug_logger.info("å°è¯•ä»akshareè·å–çœŸå®å®è§‚æ•°æ®")
                    df = self.ak.index_zh_a_hist(
                        symbol="000300",
                        period="daily",
                        start_date=(datetime.now() - timedelta(days=30)).strftime('%Y%m%d'),
                        end_date=datetime.now().strftime('%Y%m%d')
                    )

                    if not df.empty:
                        latest = df.iloc[-1]
                        result = {
                            "market_index": "æ²ªæ·±300",
                            "current_value": latest['æ”¶ç›˜'],
                            "change_pct": ((latest['æ”¶ç›˜'] - latest['å¼€ç›˜']) / latest['å¼€ç›˜']) * 100,
                            "volume": latest['æˆäº¤é‡'],
                            "date": latest['æ—¥æœŸ'],
                            "data_source": "akshare"
                        }
                        end_time = datetime.now()
                        execution_time = (end_time - start_time).total_seconds()
                        self.logger.info(f"å®è§‚æ•°æ®è·å–æˆåŠŸ: æ²ªæ·±300, data_size=1, execution_time={execution_time:.2f}s, data_source=akshare")
                        return result
                except Exception as e:
                    self.logger.error(f"akshareè·å–å®è§‚æ•°æ®å¤±è´¥: {e}")
                    raise RuntimeError(f"æ— æ³•è·å–çœŸå®å®è§‚æ•°æ®: {e}")
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"å®è§‚æ•°æ®è·å–å¤±è´¥: æ²ªæ·±300, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"è·å–å®è§‚æ•°æ®å¤±è´¥: {e}")
            return {"market_index": "æ²ªæ·±300", "error": str(e)}

    def get_sector_data(self, sector: str) -> Dict[str, Any]:
        """è·å–è¡Œä¸šæ•°æ®"""
        start_time = datetime.now()
        self.debug_logger.debug(f"è·å–è¡Œä¸šæ•°æ®å¼€å§‹: sector={sector}, timestamp={start_time.isoformat()}")

        try:
            # è·å–è¡Œä¸šæ¿å—æ•°æ®
            df = ak.stock_board_industry_name_em()
            if sector in df['æ¿å—åç§°'].values:
                sector_info = df[df['æ¿å—åç§°'] == sector].iloc[0]
                result = {
                    "sector_name": sector,
                    "change_pct": sector_info['æ¶¨è·Œå¹…'],
                    "turnover": sector_info['æ¢æ‰‹ç‡'],
                    "leader_stocks": sector_info['é¢†æ¶¨è‚¡ç¥¨']
                }
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                self.logger.info(f"è¡Œä¸šæ•°æ®è·å–æˆåŠŸ: sector={sector}, data_size=1, execution_time={execution_time:.2f}s, data_source=akshare")
                return result
            else:
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                self.logger.info(f"è¡Œä¸šæ•°æ®è·å–æˆåŠŸ: sector={sector}, data_size=0, execution_time={execution_time:.2f}s, status=æœªæ‰¾åˆ°è¯¥è¡Œä¸š")
                return {"sector_name": sector, "error": "æœªæ‰¾åˆ°è¯¥è¡Œä¸š"}
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            self.logger.info(f"è¡Œä¸šæ•°æ®è·å–å¤±è´¥: sector={sector}, execution_time={execution_time:.2f}s, error={str(e)}")
            self.logger.error(f"è·å–è¡Œä¸šæ•°æ®å¤±è´¥ {sector}: {e}")
            return {"sector_name": sector, "error": str(e)}

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        start_time = datetime.now()
        cache_size = len(self._cache)
        self._cache.clear()
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        self.debug_logger.debug(f"ç¼“å­˜æ¸…ç†å®Œæˆ: previous_cache_size={cache_size}, execution_time={execution_time}")
        self.logger.info("æ•°æ®æºç¼“å­˜å·²æ¸…ç©º")

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "cache_size": len(self._cache),
            "cache_ttl": self._cache_ttl,
            "cached_functions": list(set(str(key).split('_')[0] for key in self._cache.keys()))
        }
        self.debug_logger.debug(f"ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯: {stats}")
        return stats


# å…¨å±€æ•°æ®æºé€‚é…å™¨å®ä¾‹
data_adapter = DataSourceAdapter()


def get_data_adapter() -> DataSourceAdapter:
    """è·å–å…¨å±€æ•°æ®æºé€‚é…å™¨å®ä¾‹"""
    return data_adapter


# CrewAIå·¥å…·åŒ…è£…å™¨
class DataRetrievalTool(BaseTool):
    """æ•°æ®æ£€ç´¢å·¥å…·åŸºç±»"""

    def __init__(self, adapter: DataSourceAdapter, name: str, description: str):
        # ä½¿ç”¨å±æ€§é¿å…è¢«CrewAI BaseToolè¿‡æ»¤
        self._adapter = adapter
        super().__init__(name=name, description=description)

    def get_adapter(self) -> DataSourceAdapter:
        """è·å–æ•°æ®é€‚é…å™¨"""
        # CrewAIå¯èƒ½è¿‡æ»¤æ‰äº†_adapterå±æ€§ï¼Œæˆ‘ä»¬éœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–
        if hasattr(self, '_adapter'):
            return self._adapter
        else:
            # å¦‚æœ_adapterä¸å­˜åœ¨ï¼Œä½¿ç”¨å…¨å±€æ•°æ®é€‚é…å™¨
            from crewai_system.src.tools.data_sources import get_data_adapter
            return get_data_adapter()

    @property
    def logger(self):
        """è·å–æ—¥å¿—è®°å½•å™¨"""
        return get_logger("data")

    def _run(self, **kwargs) -> Any:
        """å·¥å…·æ‰§è¡Œæ–¹æ³•"""
        try:
            return self.execute(**kwargs)
        except Exception as e:
            self.logger.error(f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": str(e)}

    def execute(self, **kwargs) -> Any:
        """å…·ä½“çš„æ‰§è¡Œé€»è¾‘ï¼Œå­ç±»å¿…é¡»å®ç°"""
        raise NotImplementedError


class PriceHistoryTool(DataRetrievalTool):
    """ä»·æ ¼å†å²è·å–å·¥å…·"""

    def __init__(self, adapter: DataSourceAdapter):
        super().__init__(adapter, "Get Price History", "è·å–è‚¡ç¥¨ä»·æ ¼å†å²æ•°æ®")

    def _run(self, ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
        """æ‰§è¡Œä»·æ ¼å†å²è·å–"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œä»·æ ¼å†å²è·å–: {ticker} ({start_date} - {end_date})")
        result = self.get_adapter().get_price_history_safe(ticker, start_date, end_date)
        self.logger.info(f"ä»·æ ¼å†å²è·å–å®Œæˆ: {ticker}, æ•°æ®æ¡æ•°: {len(result)}")
        return result


class FinancialMetricsTool(DataRetrievalTool):
    """è´¢åŠ¡æŒ‡æ ‡è·å–å·¥å…·"""

    def __init__(self, adapter: DataSourceAdapter):
        super().__init__(adapter, "Get Financial Metrics", "è·å–è‚¡ç¥¨è´¢åŠ¡æŒ‡æ ‡æ•°æ®")

    def _run(self, ticker: str) -> Dict[str, Any]:
        """æ‰§è¡Œè´¢åŠ¡æŒ‡æ ‡è·å–"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œè´¢åŠ¡æŒ‡æ ‡è·å–: {ticker}")
        result = self.get_adapter().get_financial_metrics_safe(ticker)
        self.logger.info(f"è´¢åŠ¡æŒ‡æ ‡è·å–å®Œæˆ: {ticker}, æŒ‡æ ‡æ•°é‡: {len(result)}")
        return result


class MarketDataTool(DataRetrievalTool):
    """å¸‚åœºæ•°æ®è·å–å·¥å…·"""

    def __init__(self, adapter: DataSourceAdapter):
        super().__init__(adapter, "Get Market Data", "è·å–è‚¡ç¥¨å¸‚åœºæ•°æ®")

    def _run(self, ticker: str) -> Dict[str, Any]:
        """æ‰§è¡Œå¸‚åœºæ•°æ®è·å–"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œå¸‚åœºæ•°æ®è·å–: {ticker}")
        result = self.get_adapter().get_market_data_safe(ticker)
        self.logger.info(f"å¸‚åœºæ•°æ®è·å–å®Œæˆ: {ticker}")
        return result


class NewsSearchTool(DataRetrievalTool):
    """æ–°é—»æœç´¢å·¥å…·"""

    def __init__(self, adapter: DataSourceAdapter):
        super().__init__(adapter, "Search Financial News", "æœç´¢è´¢ç»æ–°é—»")

    def _run(self, keywords: str, num_articles: int = 10) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ–°é—»æœç´¢"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œæ–°é—»æœç´¢: {keywords}, æ•°é‡: {num_articles}")
        result = self.get_adapter().search_financial_news_safe(keywords, num_articles)
        self.logger.info(f"æ–°é—»æœç´¢å®Œæˆ: {keywords}, æœç´¢åˆ° {len(result)} æ¡æ–°é—»")
        return result


class MacroDataTool(DataRetrievalTool):
    """å®è§‚æ•°æ®è·å–å·¥å…·"""

    def __init__(self, adapter: DataSourceAdapter):
        super().__init__(adapter, "Get Macro Data", "è·å–å®è§‚ç»æµæ•°æ®")

    def _run(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®è§‚æ•°æ®è·å–"""
        self.logger.info("å¼€å§‹æ‰§è¡Œå®è§‚æ•°æ®è·å–")
        result = self.get_adapter().get_macro_data()
        self.logger.info(f"å®è§‚æ•°æ®è·å–å®Œæˆ: {result.get('market_index', 'æœªçŸ¥')}")
        return result